name: Optimized CI Pipeline

on:
  push:
    branches: [main, develop, test-fixes]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  GRADLE_OPTS: -Xmx4g -XX:MaxMetaspaceSize=1g -XX:+UseG1GC
  _JAVA_OPTIONS: -Djava.awt.headless=true

jobs:
  quick-validation:
    name: Quick Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'
        cache: gradle

    - name: Make scripts executable
      run: |
        chmod +x gradlew
        chmod +x library/scripts/*.py || true || true
        # Make .sh files executable if they exist
        find library/scripts -name "*.sh" -type f -exec chmod +x {} \; 2>/dev/null || true

    - name: Compile and quick test
      run: |
        ./gradlew clean :library:compileJava :library:compileTestJava --no-daemon
        echo "✅ Compilation successful!"
      env:
        SPRING_PROFILES_ACTIVE: ci

    - name: Upload quick test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: quick-test-results
        path: test-results-*.json

  unit-tests:
    name: Unit Tests
    needs: quick-validation
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      matrix:
        batch: [1, 2, 3, 4, 5]

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'
        cache: gradle

    - name: Make scripts executable
      run: |
        chmod +x gradlew
        chmod +x library/scripts/*.py || true

    - name: Run unit test batch ${{ matrix.batch }}
      run: |
        # Define test patterns for each batch
        case ${{ matrix.batch }} in
          1) PATTERN="io.github.jspinak.brobot.action.*" ;;
          2) PATTERN="io.github.jspinak.brobot.navigation.*" ;;
          3) PATTERN="io.github.jspinak.brobot.state*" ;;
          4) PATTERN="io.github.jspinak.brobot.util.*" ;;  # Changed from logging to util
          5) PATTERN="io.github.jspinak.brobot.*" ;;  # Catch-all
        esac

        python3 library/scripts/run-tests-safe.py library --pattern "$PATTERN" --batch-size 20 --timeout 30
      env:
        SPRING_PROFILES_ACTIVE: ci
        CI: true

    - name: Upload batch ${{ matrix.batch }} results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results-batch-${{ matrix.batch }}
        path: test-results-*.json

  integration-tests:
    name: Integration Tests
    needs: quick-validation
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4

    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'
        cache: gradle

    - name: Set up virtual display
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb
        export DISPLAY=:99
        Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &

    - name: Make scripts executable
      run: |
        chmod +x gradlew
        chmod +x library/scripts/*.py || true

    - name: Run integration tests
      run: |
        export DISPLAY=:99

        # First compile library-test
        ./gradlew :library-test:compileTestJava --no-daemon || true

        # Run integration tests if compilation succeeded
        if [ $? -eq 0 ]; then
          python3 library/scripts/run-tests-safe.py library-test --batch-size 10 --timeout 60
        else
          echo "Skipping integration tests due to compilation issues"
        fi
      env:
        SPRING_PROFILES_ACTIVE: ci
        CI: true

    - name: Upload integration test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: integration-test-results
        path: test-results-*.json

  test-summary:
    name: Test Summary
    needs: [unit-tests, integration-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - uses: actions/checkout@v4

    - name: Download all test results
      uses: actions/download-artifact@v4
      with:
        path: test-artifacts

    - name: Aggregate test results
      run: |
        # Find all JSON result files
        find test-artifacts -name "*.json" -type f > result-files.txt

        # Create aggregation script
        cat > aggregate.py << 'EOF'
        import json
        import sys
        from pathlib import Path

        total_passed = 0
        total_failed = 0
        total_timeout = 0
        all_failed = []

        with open('result-files.txt', 'r') as f:
            for line in f:
                filepath = line.strip()
                if not filepath:
                    continue
                try:
                    with open(filepath, 'r') as jf:
                        data = json.load(jf)
                        summary = data.get('summary', {})
                        total_passed += summary.get('passed', 0)
                        total_failed += summary.get('failed', 0)
                        total_timeout += summary.get('skipped', 0)

                        for result in data.get('results', []):
                            if result.get('status') == 'failed':
                                all_failed.append(result.get('class', 'Unknown'))
                except Exception as e:
                    print(f"Error processing {filepath}: {e}", file=sys.stderr)

        total = total_passed + total_failed + total_timeout

        print("# Test Execution Summary\n")
        print(f"**Total Tests Run**: {total}")
        print(f"**Passed**: {total_passed} ✅")
        print(f"**Failed**: {total_failed} ❌")
        print(f"**Timeout/Skipped**: {total_timeout} ⏱️")

        if total > 0:
            success_rate = (total_passed * 100) // total
            print(f"**Success Rate**: {success_rate}%")

            if success_rate >= 95:
                print("\n## Status: ✅ SUCCESS")
            elif success_rate >= 80:
                print("\n## Status: ⚠️ UNSTABLE")
            else:
                print("\n## Status: ❌ FAILURE")

        if all_failed:
            print("\n## Failed Tests\n")
            for test in all_failed[:20]:
                print(f"- `{test}`")
            if len(all_failed) > 20:
                print(f"\n*... and {len(all_failed) - 20} more*")

        # Set exit code based on results
        if total_failed > 0:
            sys.exit(1)
        EOF

        python3 aggregate.py >> $GITHUB_STEP_SUMMARY

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync(process.env.GITHUB_STEP_SUMMARY, 'utf8');

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

  # Performance check job removed - no actual performance tests exist
