"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7315],{28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var s=t(96540);const a={},r=s.createContext(a);function i(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(r.Provider,{value:n},e.children)}},81512:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"integrations/mcp-server/examples","title":"AI Integration Examples","description":"Learn how to integrate popular AI services and frameworks with the Brobot MCP Server.","source":"@site/docs/06-integrations/mcp-server/examples.md","sourceDirName":"06-integrations/mcp-server","slug":"/integrations/mcp-server/examples","permalink":"/brobot/docs/integrations/mcp-server/examples","draft":false,"unlisted":false,"editUrl":"https://github.com/jspinak/brobot/edit/main/docs/docs/06-integrations/mcp-server/examples.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"docSidebar","previous":{"title":"API Reference","permalink":"/brobot/docs/integrations/mcp-server/api-reference"},"next":{"title":"Troubleshooting Guide","permalink":"/brobot/docs/integrations/mcp-server/troubleshooting"}}');var a=t(74848),r=t(28453);const i={sidebar_position:5},o="AI Integration Examples",c={},l=[{value:"OpenAI GPT Integration",id:"openai-gpt-integration",level:2},{value:"Basic GPT-4 Integration",id:"basic-gpt-4-integration",level:3},{value:"Vision-Enabled GPT-4V",id:"vision-enabled-gpt-4v",level:3},{value:"Anthropic Claude Integration",id:"anthropic-claude-integration",level:2},{value:"Claude 3 with Computer Use",id:"claude-3-with-computer-use",level:3},{value:"Interactive Claude Assistant",id:"interactive-claude-assistant",level:3},{value:"LangChain Integration",id:"langchain-integration",level:2},{value:"Brobot as LangChain Tool",id:"brobot-as-langchain-tool",level:3},{value:"Custom LangChain Chain",id:"custom-langchain-chain",level:3},{value:"AutoGPT/Agent Frameworks",id:"autogptagent-frameworks",level:2},{value:"AutoGPT Plugin",id:"autogpt-plugin",level:3},{value:"Multi-Agent Systems",id:"multi-agent-systems",level:2},{value:"Coordinator-Worker Pattern",id:"coordinator-worker-pattern",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Error Handling",id:"1-error-handling",level:3},{value:"2. State Verification",id:"2-state-verification",level:3},{value:"3. Context Management",id:"3-context-management",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Caching AI Decisions",id:"caching-ai-decisions",level:3},{value:"Parallel Processing",id:"parallel-processing",level:3},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"ai-integration-examples",children:"AI Integration Examples"})}),"\n",(0,a.jsx)(n.p,{children:"Learn how to integrate popular AI services and frameworks with the Brobot MCP Server."}),"\n",(0,a.jsx)(n.h2,{id:"openai-gpt-integration",children:"OpenAI GPT Integration"}),"\n",(0,a.jsx)(n.h3,{id:"basic-gpt-4-integration",children:"Basic GPT-4 Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import openai\nfrom brobot_client import BrobotClient\nimport json\n\n# Initialize clients\nopenai.api_key = "your-api-key"\nbrobot = BrobotClient()\n\ndef execute_natural_language_command(instruction: str):\n    """Execute a natural language command using GPT-4."""\n    \n    # Get current screen state\n    observation = brobot.get_observation()\n    active_states = [s.name for s in observation.active_states]\n    \n    # Create prompt for GPT-4\n    prompt = f"""\n    Current application state: {active_states}\n    User instruction: {instruction}\n    \n    Available actions:\n    - click(image_pattern) - Click on UI element\n    - type_text(text) - Type text\n    - wait_for_state(state_name) - Wait for state\n    \n    Respond with a JSON array of actions to execute.\n    Example: [{"action": "click", "params": {"pattern": "login_btn.png"}}]\n    """\n    \n    # Get GPT-4 response\n    response = openai.ChatCompletion.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": "You are a UI automation assistant."},\n            {"role": "user", "content": prompt}\n        ],\n        temperature=0.3\n    )\n    \n    # Parse and execute actions\n    actions = json.loads(response.choices[0].message.content)\n    \n    for action in actions:\n        if action["action"] == "click":\n            brobot.click(action["params"]["pattern"])\n        elif action["action"] == "type_text":\n            brobot.type_text(action["params"]["text"])\n        elif action["action"] == "wait_for_state":\n            brobot.wait_for_state(action["params"]["state"])\n\n# Example usage\nexecute_natural_language_command("Log into the application with username \'demo\'")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"vision-enabled-gpt-4v",children:"Vision-Enabled GPT-4V"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import base64\nfrom openai import OpenAI\n\nclient = OpenAI()\nbrobot = BrobotClient()\n\ndef analyze_and_act():\n    """Use GPT-4V to analyze screenshots and decide actions."""\n    \n    # Get observation with screenshot\n    obs = brobot.get_observation()\n    \n    # Prepare image for GPT-4V\n    image_base64 = obs.screenshot\n    \n    response = client.chat.completions.create(\n        model="gpt-4-vision-preview",\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "text",\n                        "text": "What UI elements do you see? What should I click to login?"\n                    },\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": f"data:image/png;base64,{image_base64}"\n                        }\n                    }\n                ]\n            }\n        ],\n        max_tokens=300\n    )\n    \n    # Execute suggested action\n    suggestion = response.choices[0].message.content\n    print(f"GPT-4V suggests: {suggestion}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"anthropic-claude-integration",children:"Anthropic Claude Integration"}),"\n",(0,a.jsx)(n.h3,{id:"claude-3-with-computer-use",children:"Claude 3 with Computer Use"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from anthropic import Anthropic\nfrom brobot_client import BrobotClient\nimport asyncio\n\nanthropic = Anthropic(api_key="your-api-key")\nbrobot = BrobotClient()\n\nclass ClaudeAutomationAgent:\n    """Agent that uses Claude to control applications."""\n    \n    def __init__(self):\n        self.conversation = []\n        \n    async def process_task(self, task: str):\n        """Process a high-level task using Claude."""\n        \n        # Get current state\n        obs = brobot.get_observation()\n        \n        # Build context\n        context = f"""\n        Task: {task}\n        Current screen: {obs.active_states}\n        Available actions: click, type, drag, wait\n        \n        Plan and execute the steps needed to complete this task.\n        """\n        \n        response = anthropic.messages.create(\n            model="claude-3-opus",\n            messages=[{"role": "user", "content": context}],\n            max_tokens=1000\n        )\n        \n        # Execute Claude\'s plan\n        await self._execute_plan(response.content)\n    \n    async def _execute_plan(self, plan: str):\n        """Parse and execute Claude\'s plan."""\n        # Implementation depends on Claude\'s response format\n        pass\n\n# Usage\nagent = ClaudeAutomationAgent()\nawait agent.process_task("Create a new document and save it as \'report.pdf\'")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"interactive-claude-assistant",children:"Interactive Claude Assistant"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def create_interactive_assistant():\n    """Create an interactive automation assistant with Claude."""\n    \n    class InteractiveSession:\n        def __init__(self):\n            self.messages = []\n            \n        def chat(self, user_input: str):\n            # Add context about current screen\n            obs = brobot.get_observation()\n            \n            enhanced_input = f"""\n            User: {user_input}\n            \n            Current application state: {obs.get_most_confident_state().name}\n            Visible elements: {[s.name for s in obs.active_states]}\n            """\n            \n            self.messages.append({"role": "user", "content": enhanced_input})\n            \n            response = anthropic.messages.create(\n                model="claude-3-sonnet",\n                messages=self.messages\n            )\n            \n            self.messages.append({"role": "assistant", "content": response.content})\n            \n            return response.content\n    \n    return InteractiveSession()\n\n# Interactive usage\nsession = create_interactive_assistant()\nprint(session.chat("How do I navigate to settings?"))\nprint(session.chat("Now change the theme to dark mode"))\n'})}),"\n",(0,a.jsx)(n.h2,{id:"langchain-integration",children:"LangChain Integration"}),"\n",(0,a.jsx)(n.h3,{id:"brobot-as-langchain-tool",children:"Brobot as LangChain Tool"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom brobot_client import BrobotClient\n\n# Create Brobot tools for LangChain\ndef create_brobot_tools():\n    client = BrobotClient()\n    \n    def observe_screen(query: str = "") -> str:\n        """Observe current screen state."""\n        obs = client.get_observation()\n        states = [f"{s.name} ({s.confidence:.0%})" for s in obs.active_states]\n        return f"Active states: {\', \'.join(states)}"\n    \n    def click_element(pattern: str) -> str:\n        """Click on a UI element."""\n        try:\n            result = client.click(pattern)\n            return f"Clicked {pattern} successfully"\n        except Exception as e:\n            return f"Failed to click {pattern}: {str(e)}"\n    \n    def type_text(text: str) -> str:\n        """Type text in current field."""\n        result = client.type_text(text)\n        return f"Typed \'{text}\'"\n    \n    return [\n        Tool(\n            name="ObserveScreen",\n            func=observe_screen,\n            description="Get current screen state and active UI elements"\n        ),\n        Tool(\n            name="Click",\n            func=click_element,\n            description="Click on UI element by image pattern name"\n        ),\n        Tool(\n            name="Type",\n            func=type_text,\n            description="Type text into current field"\n        )\n    ]\n\n# Create agent\nllm = OpenAI(temperature=0)\ntools = create_brobot_tools()\nagent = initialize_agent(tools, llm, agent="zero-shot-react-description", verbose=True)\n\n# Use agent\nagent.run("Log into the application with username \'demo@example.com\'")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"custom-langchain-chain",children:"Custom LangChain Chain"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.memory import ConversationBufferMemory\n\nclass BrobotAutomationChain:\n    """Custom chain for complex automations."""\n    \n    def __init__(self):\n        self.brobot = BrobotClient()\n        self.memory = ConversationBufferMemory()\n        \n        self.planner_prompt = PromptTemplate(\n            input_variables=["task", "current_state"],\n            template="""\n            Task: {task}\n            Current State: {current_state}\n            \n            Create a step-by-step plan to complete this task.\n            Format: numbered list of actions\n            """\n        )\n        \n        self.planner = LLMChain(\n            llm=OpenAI(temperature=0.3),\n            prompt=self.planner_prompt,\n            memory=self.memory\n        )\n    \n    def execute_task(self, task: str):\n        # Get current state\n        obs = self.brobot.get_observation()\n        current_state = obs.get_most_confident_state().name\n        \n        # Generate plan\n        plan = self.planner.run(task=task, current_state=current_state)\n        \n        # Execute plan steps\n        for step in plan.split(\'\\n\'):\n            if \'click\' in step.lower():\n                # Extract pattern and click\n                pass\n            elif \'type\' in step.lower():\n                # Extract text and type\n                pass\n\n# Usage\nchain = BrobotAutomationChain()\nchain.execute_task("Create a new invoice for $1,500")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"autogptagent-frameworks",children:"AutoGPT/Agent Frameworks"}),"\n",(0,a.jsx)(n.h3,{id:"autogpt-plugin",children:"AutoGPT Plugin"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class BrobotAutoGPTPlugin:\n    """Plugin to give AutoGPT UI control capabilities."""\n    \n    def __init__(self):\n        self.client = BrobotClient()\n        \n    def get_commands(self):\n        return {\n            "ui_observe": self.observe,\n            "ui_click": self.click,\n            "ui_type": self.type_text,\n            "ui_wait": self.wait_for_state\n        }\n    \n    def observe(self) -> dict:\n        """Observe current UI state."""\n        obs = self.client.get_observation()\n        return {\n            "states": [s.name for s in obs.active_states],\n            "screenshot_available": bool(obs.screenshot)\n        }\n    \n    def click(self, target: str) -> dict:\n        """Click UI element."""\n        try:\n            self.client.click(target)\n            return {"success": True, "message": f"Clicked {target}"}\n        except Exception as e:\n            return {"success": False, "error": str(e)}\n    \n    def type_text(self, text: str) -> dict:\n        """Type text."""\n        self.client.type_text(text)\n        return {"success": True, "message": f"Typed: {text}"}\n    \n    def wait_for_state(self, state: str, timeout: float = 10) -> dict:\n        """Wait for specific state."""\n        try:\n            self.client.wait_for_state(state, timeout)\n            return {"success": True, "message": f"Reached state: {state}"}\n        except:\n            return {"success": False, "error": "Timeout waiting for state"}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"multi-agent-systems",children:"Multi-Agent Systems"}),"\n",(0,a.jsx)(n.h3,{id:"coordinator-worker-pattern",children:"Coordinator-Worker Pattern"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom typing import List, Dict\n\nclass AutomationCoordinator:\n    """Coordinates multiple AI agents for complex tasks."""\n    \n    def __init__(self):\n        self.brobot = BrobotClient()\n        self.observer_agent = ObserverAgent()\n        self.planner_agent = PlannerAgent()\n        self.executor_agent = ExecutorAgent()\n    \n    async def execute_complex_task(self, task: str):\n        # Observer analyzes current state\n        state_analysis = await self.observer_agent.analyze(self.brobot)\n        \n        # Planner creates execution plan\n        plan = await self.planner_agent.create_plan(task, state_analysis)\n        \n        # Executor carries out plan\n        results = await self.executor_agent.execute(plan, self.brobot)\n        \n        return results\n\nclass ObserverAgent:\n    """Specialized in understanding UI state."""\n    \n    async def analyze(self, brobot: BrobotClient) -> Dict:\n        obs = brobot.get_observation()\n        \n        # Use AI to analyze screenshot and states\n        analysis = {\n            "current_screen": self._identify_screen(obs),\n            "available_actions": self._find_actionable_elements(obs),\n            "navigation_options": self._identify_navigation(obs)\n        }\n        \n        return analysis\n\nclass PlannerAgent:\n    """Creates execution plans."""\n    \n    async def create_plan(self, task: str, state: Dict) -> List[Dict]:\n        # Use AI to create step-by-step plan\n        pass\n\nclass ExecutorAgent:\n    """Executes plans reliably."""\n    \n    async def execute(self, plan: List[Dict], brobot: BrobotClient):\n        results = []\n        for step in plan:\n            result = await self._execute_step(step, brobot)\n            results.append(result)\n            \n            if not result["success"]:\n                # Handle failures\n                break\n        \n        return results\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsx)(n.h3,{id:"1-error-handling",children:"1. Error Handling"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def safe_automation(instruction: str):\n    """Automation with comprehensive error handling."""\n    max_retries = 3\n    \n    for attempt in range(max_retries):\n        try:\n            # Get current state\n            obs = brobot.get_observation()\n            \n            # AI processes instruction\n            actions = ai_process_instruction(instruction, obs)\n            \n            # Execute with validation\n            for action in actions:\n                result = execute_action(action)\n                if not result.success:\n                    # AI decides how to recover\n                    recovery = ai_plan_recovery(action, result.error)\n                    execute_action(recovery)\n            \n            return True\n            \n        except Exception as e:\n            if attempt < max_retries - 1:\n                # Let AI decide if we should retry\n                should_retry = ai_should_retry(e, attempt)\n                if not should_retry:\n                    break\n            else:\n                raise\n'})}),"\n",(0,a.jsx)(n.h3,{id:"2-state-verification",children:"2. State Verification"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def verify_state_transition(expected_state: str, timeout: float = 10):\n    """Verify state transitions with AI assistance."""\n    start_time = time.time()\n    \n    while time.time() - start_time < timeout:\n        obs = brobot.get_observation()\n        \n        # AI verifies if we\'re in expected state\n        is_correct = ai_verify_state(obs, expected_state)\n        \n        if is_correct:\n            return True\n        \n        # AI suggests corrective action\n        correction = ai_suggest_correction(obs, expected_state)\n        if correction:\n            execute_action(correction)\n        \n        time.sleep(1)\n    \n    return False\n'})}),"\n",(0,a.jsx)(n.h3,{id:"3-context-management",children:"3. Context Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class ContextAwareAutomation:\n    """Maintains context across automation sessions."""\n    \n    def __init__(self):\n        self.context = {\n            "application": None,\n            "user": None,\n            "task_history": [],\n            "state_history": []\n        }\n        \n    def execute_with_context(self, task: str):\n        # Add current context to AI prompt\n        enhanced_task = f"""\n        Task: {task}\n        Application: {self.context[\'application\']}\n        Previous tasks: {self.context[\'task_history\'][-5:]}\n        """\n        \n        result = ai_execute(enhanced_task)\n        \n        # Update context\n        self.context[\'task_history\'].append(task)\n        return result\n'})}),"\n",(0,a.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"caching-ai-decisions",children:"Caching AI Decisions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\nimport hashlib\n\nclass CachedAIAutomation:\n    """Cache AI decisions for repeated scenarios."""\n    \n    @lru_cache(maxsize=100)\n    def get_ai_decision(self, state_hash: str, task: str):\n        """Cache AI decisions based on state and task."""\n        return ai_model.decide(state_hash, task)\n    \n    def execute_task(self, task: str):\n        obs = brobot.get_observation()\n        \n        # Create hash of current state\n        state_data = {\n            "states": [s.name for s in obs.active_states],\n            "screen_size": (obs.screen_width, obs.screen_height)\n        }\n        state_hash = hashlib.md5(\n            json.dumps(state_data, sort_keys=True).encode()\n        ).hexdigest()\n        \n        # Get cached or new decision\n        decision = self.get_ai_decision(state_hash, task)\n        \n        # Execute decision\n        return execute_decision(decision)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'async def parallel_ui_analysis():\n    """Analyze UI using multiple AI models in parallel."""\n    \n    async def gpt_analysis():\n        return await gpt_analyze_ui(brobot.get_observation())\n    \n    async def claude_analysis():\n        return await claude_analyze_ui(brobot.get_observation())\n    \n    async def local_model_analysis():\n        return await local_model_analyze(brobot.get_observation())\n    \n    # Run all analyses in parallel\n    results = await asyncio.gather(\n        gpt_analysis(),\n        claude_analysis(),\n        local_model_analysis()\n    )\n    \n    # Combine insights\n    return combine_ai_insights(results)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Explore the ",(0,a.jsx)(n.a,{href:"./api-reference",children:"API Reference"})," for detailed endpoint information"]}),"\n",(0,a.jsxs)(n.li,{children:["Read ",(0,a.jsx)(n.a,{href:"./troubleshooting",children:"Troubleshooting"})," for common issues"]}),"\n",(0,a.jsxs)(n.li,{children:["Join our ",(0,a.jsx)(n.a,{href:"https://discord.gg/brobot",children:"Discord"})," for community support"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}}}]);